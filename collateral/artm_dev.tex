\documentclass[11pt,a4paper,twoside]{report}

\begin{document}

\tableofcontents

\chapter{BigARTM - internal documentation}

\section{Intro}
ToDo: write short gentle introduction that provide links to state of the art articles on topic models and BigARTM.

\paragraph{Main goals.}
\begin{itemize}
    \item Implementation of core topic modeling algorithms with ARTM (Additively Regularized Topic Models)
    \item Prefer online algorithms, that do not require storing complete doc-word matrix in memory
    \item Utilize sparsity of doc-token and token-topic matrices
    \item Scale well for 32 CPU cores and higher, efficiently using shared memory within single process
    \item Exhibit high convergence rates
    \item Be portable (written in C/C++, tested with gcc, intel and cl.exe)
    \item Have an interface in Python, Java and $C\sharp$
    \item Be open-source (MIT license)
\end{itemize}

Initial development take place in private github repository,
but over time we plan to release this software under MIT license.
First version we release to public may not have the following features:
\begin{itemize}
    \item Distributed cluster solution
    \item CUDA and Intel Xeon Phi support
\end{itemize}
This is likely to be added in later versions.

\paragraph{Success criteria.}
When codebase meets this success criteria we are free to release the library.
\begin{itemize}
    \item The algorithm scales linearly up to 32 CPU cores when process pubmed task (http://archive.ics.uci.edu/ml/datasets/Bag+of+Words)
    \item On small datasets perplexity of our method is on parity with other libraries
    \item Builds with gcc, intel and cl.exe compiler; runs on Ubuntu, Solaris and Windows.
    \item No crashes, hangs or memory leaks in stress testing against real-world and model datasets
    \item Convergence rate is well understood
    \item Performance model is well understood (Disk, Memory, CPU)
\end{itemize}

\section{Design}

\subsection{Core modules}

Core modules are \emph{DataLoader}, \emph{Processor}, and \emph{Merger}.
Interaction between these components is orchestrated by a class called \emph{Instance},
and two queues --- \emph{processor input queue}, and \emph{processor output queue}.

\textbf{DataLoader.} Responsibility of DataLoader component is to populate the input processor queue
with batches (small chunks of the collection).
It is up to implementation of the DataLoader to decide where to gather batches
(they can be stored in memory, on disk, or came through the network).
DataLoader is expected to perform iterative scans over the whole collection.

\textbf{Processor.} Processor component withdraws index parts from the queue
and infers a distribution of the documents into topics.
The output is stored in \emph{processor output queue}.
Before processing new batch the processor asks merger about the the latest word-topic matrix.

If processor observes a token that is not part of token-token matrix, it stores this token in the list of
new ``discovered'' tokens, and transfers this list as part of processor output.
Merger picks up all such tokens, and initializes new row in token-topic matrix.
So, during the first scan over the collection the dictionary is gathered automatically.

\textbf{Merger.}
Merger reads the processor output queue and updates token-topic matrix.
At any moment there exist two copies of token-topic matrix ---
``active'' and ``background''.
The ``active'' token-topic matrix is used by processors,
while ``background'' token-topic matrix can be safely updated by merger.
It is up to merger to decide when to switch ``active'' token-topic matrix to the ``background'' one.
Even after switching ``active'' topic-matrix the previous (deprecated) active matrix
will be in use to complete all ongoing processing of batches.
Every new batch will be processed with new matrix.

This design supports multiple concurrent dataloaders, multiple concurrent processors, but only one merger.

\subsection{Concurrency and thread safety}
The system must have the following locks:
\begin{enumerate}
    \item Lock accesses to the index parts queue (accessed by dataloaders and processors)
    \item Lock accesses to doc-topic distribution queue(accessed by processors and merger)
    \item Lock for accessing word-topic matrix from processor to merger
\end{enumerate}

It is easy to understand why locks (1) and (2) are cheap:
we just assume that index parts are large enough so that the time is mostly spent in reading and processing,
not in transferring references from dataloader to processor.
Lock (3) will be very check if impose one requirement:
once the merger returns a view of word-topic matrix to processor,
this view is read-only and can be further accessed without locks.
Merger will proceed by building a new view (again without locking since it is not in use by any processor),
up to the point where new view is different enough from the old view,
and hence should be will be replaced with a new one.
This will not change the view for all running processors
until the finish current index part and request new view from merger.
Therefore, lock (3) is only acquired when

a) merger updates the “current view” of world-topic matrix, and

b) processor finishes index part, and requests new view of world-topic matrix.

All locks (1), (2), (3) can be implemented as usual boost locks.

This design can be extended to cluster environment by implementing a networkmerger,
which in addition to merging word-topic matrix within local process should merge it with updates,
coming from networkmergers from other nodes.
This design can also utilize CUDA-enabled devices or Index Xeon Phi co-processor
by implementing a special processor (without changing the rest of architecture).
For CUDA the most promising parallelization is to assign CUDA-threads to topic
while inferring a doc-topic distribution.
dataloader might implement caching, so that if the whole collection fits into memory
it doesn’t have to reload index parts from disk for the second and further scans.

Last but not least:
we assume that the library is used from a single thread
(e.g. no calls to the library API methods happen in parallel).

\subsection{Data layout}
\begin{enumerate}
    \item Doc-word matrix: each doc is represented as list of word ids and their counts
    \item Word-topic matrix: each word is represented the list of topics and their probabilities
    \item Doc-topic matrix: each doc is represented as a sequential vector of topics (no sparsity)
\end{enumerate}

Due to caching in CPU it is important to have topics as a minor dimension in both word-topic and doc-topic matrices.

\section{FAQ}

\subsection{What is ``google protocol buffers''}

For an overall introduction about google protocol buffers read the tutorial:

https://developers.google.com/protocol-buffers/docs/cpptutorial

Now take a look at file ``messages.proto'' under src/topicmd/,
and a compiled version (other messages.* files).

messages.proto describes key objects that users can pass into the library.
For example, it defines the following entities:
\begin{itemize}
    \item the format of input data
        (e.g. the format to represent an arbitrary collection of textual files),
    \item all configuration parameters (number of concurrent processors, algorithm to use, etc),
    \item the format of output topic models
 \end{itemize}

For example, collection is represented as a set of Items.
Items is what you normally call ``documents'' - they have a list of tokens (aka terms),
together with the number of occurrences of those tokens.
All tokens in each Item are organized into Field (for example, ``body'', ``author'', ``title'', ``year'').
This is a nice way of incorporating metadata into the document.

When passing items into the library those items should be organized into Batches.
Each batch is a collection of items, that share common dictionary of tokens.
Then each items is represented as two vectors - indexes of tokens in the dictionary,
and the corresponding occurences.

\paragraph{Key benefits of google protofol buffers}

Once you define you objects in .proto file, you can use them in many programming languages.
Google officially supports C++, Python, and Java. There are also good implementation for $C\sharp$.
I've also heard that it might be something for matlab, but I'm not so sure. Here is how it works:

1. Each proto-message has a serializer, which converts the message into byte array
(and corresponding deserializer that restores an original message).
The clue is that object can be serialized in one language (for example, in Python),
and deserialized in another language (for example, in C++).

2. Passing byte arrays between different languages is very simple.
If you have a .dll (or .so library in Linux) with "extern c" api,
you may call methods in this library from other languages.
I've done that from $C\sharp$, Python and Matlab,
and this is the most portable solution.

\subsection{There is so much code.. Where should I start?!}

First, take a look at cpp\_client/srcmain.cc.
It is an example of ``external application build on top of our library'' "---
it loads a collection of texts from a file,
splits this collections into chunks ("partitions"),
and sends those chunks into the library.
Then the library tunes a model,
returns it back  cpp\_client,
and the client reports top N words in each topic.

Try to run cpp\_client step by step in debugger and see what exactly it does.

\section{Algorithms}

\subsection{Online PLSA}
Online PLSA is the only method implemented at the moment.
It has the following implementation details:
\begin{enumerate}
    \item Processing of documents happens in parallel, according to the design
    \item Strategy of updating matrix $\Phi$: whenever Merger finishes processing of the next Processors output,
    it updates matrix $\Phi$. It will be picked by each processor whenever it starts processing next partition.
    \item Matrix Phi is initialized with random [0..1] values.
    \item Algorithm is truly online. Distributions $\theta_{t d}$ are inferred from scratch on every scan of the collection (Processors currently don't have a way to store the $\Theta$ values from previous iteration,
        and re-use these values in later iteration)
    \item Counters $n_{w t}$ and $n_t$ keep increasing, even between scans of the whole collection.
    No exponential decay is currently implemented.
    \item Tuning the model is a background task. The only supported stop-criteria is the number of processed documents. The simplest way to express this is to say "stop after N*K documents", when N is the size of collection,
        and K is the desired number of scans.
\end{enumerate}

\section{How to build sources}

\subsection{Windows}

\begin{enumerate}
\item Register a user on https://github.com and send login to sashafrey@gmail.com.
\item Install msysgit:
	https://msysgit.googlecode.com/files/Git-1.8.5.2-preview20131230.exe
   During installation select "Run Git from the Windows Command Prompt"
   (then you will be able to use git from any cmd window)
\item git clone https://github.com/sashafrey/topicmod
\item Download and unpack boost 1.55 (http://sourceforge.net/projects/boost/files/)
   Set environmental variable BOOST\_ROOT to the root of our boost installation. To do so you may use cmd.exe:
 \begin{verbatim}
 	setx BOOST_ROOT "C:\\Program Files\\boost\\boost_1_55_0"
 \end{verbatim}
\item Download and unpack protobuf 2.5.0 (https://protobuf.googlecode.com/files/protobuf-2.5.0.zip).
   Set environmental variable PROTOBUF\_ROOT to the root protocol buffers library.
  \begin{verbatim}
	setx PROTOBUF_ROOT "C:\\Program Files\\protobuf\\src"
  \end{verbatim}
\item Install Visual Studio 2010 + SP1, or Visual Studio 2012.
\item Configure Visual Studio to use spaces instead of tabs; tab is 2 spaces.
    To do so, open Tools / Text Editor / All languages / Tabs, and select the following:
    \begin{itemize}
        \item Indenting "--- smart,
        \item Tab size "--- 2,
        \item Indent size "--- 2,
        \item Select "insert spaces"
    \end{itemize}
\item Open root/src/topicmd.sln
\item Build all projects (debug or release, Win32) and execute tests. No 64-bit builds yet.
\end{enumerate}


{\bf Note: if you use VS2010, you must install Service Pack 1.}

\subsection{Linux}

Use Makefiles under /src/topicmd and /src/cpp\_client.

\section{Organizational stuff}

\subsection{Code style}
In the code we follow google code style, with the following changes:
\begin{enumerate}
    \item exceptions are allowed
\end{enumerate}

\subsection{Branching strategy.}
Every new feature in a separate branch.
Then Integrator merges this feature branches into the master branch.
The key responsibility of Integrator is to ensure stability of the master branch:
\begin{itemize}
    \item No build failures on Windows and on Linux
    \item All unit test passes
    \item Documentation is updated
    \item Verify code style
\end{itemize}
Everyone should periodically merge master branch into their feature branches.

\end{document} 