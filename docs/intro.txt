Introduction
============

This is the documentation for the BigARTM library.
BigARTM is a tool to infer `topic models`_,
based on a novel technique called
`Additive Regularization of Topic Models`_.
This technique effectively builds multi-objective models
by adding the weighted sums of regularizers to the optimization criterion.
BigARTM is known to combine well
very different objectives, including sparsing, smoothing and topics decorrelation.
Such combination of regularizers significantly improves
several quality measures almost without any loss of the perplexity.

.. sidebar:: Getting help

  Having trouble? We'd like to help!

  * Try the :doc:`FAQ <faq>` -- it's got answers to many common questions.

  * Looking for specific information? Try the :ref:`genindex`,
    :ref:`search` or :ref:`glossary`.

  * Search for information in the archives of the `artm-users`_ mailing list, or
    `post a question`_.

  * Report bugs with BigARTM in our `ticket tracker`_.

  .. _artm-users: https://groups.google.com/group/artm-users
  .. _post a question: https://groups.google.com/d/forum/artm-users
  .. _ticket tracker: https://github.com/sashafrey/topicmod/issues

Let's list the key features of the library.

**Online**.
BigARTM never stores the entire text collection
in the main memory. Instead the collection is split into
small subsets called 'batches', and loads a limited
number of batches at a time.

**Parallel**.
BigARTM can concurrently process several batches,
and by doing so it substantially improves the throughput
on multi-core machines. By placing all processing into
a single process BigARTM achieves high level of concurrency
without consuming any additional memory.

**Distributed**.
BigARTM is able to distribute all CPU-intensive
processing to several machines, connected by network.
It also supports a special ``proxy`` mode,
which allows you to perform experiments on a remote cluster.

**Extensible API**.
BigARTM comes with an API in Python,
but can be easily extended for all other languages
that have an implementation of `Google Protocol Buffers`_.

**Cross-platform**.
BigARTM is known to be compatible with gcc,
clang and the Microsoft
compiler (VS 2012). We have tested our library on Windows, Ubuntu
and Fedora.

**Open source**.
BigARTM is released under the `New BSD License`_.
If you plan to use our library commercially, please beware that
BigARTM depends on ZeroMQ. Please, make sure to review
`ZeroMQ license`_.

.. _Additive Regularization of Topic Models: http://www.machinelearning.ru/wiki/images/1/1f/Voron14aist.pdf
.. _topic models: http://en.wikipedia.org/wiki/Topic_model
.. _Google Protocol Buffers: https://code.google.com/p/protobuf/
.. _New BSD license: http://opensource.org/licenses/BSD-3-Clause
.. _ZeroMQ license: http://zeromq.org/area:licensing
